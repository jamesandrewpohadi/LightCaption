{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco exists, build coco...\n",
      "python setup.py build_ext --inplace\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> pycocotools\n",
      "rm -rf build\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib.linux-x86_64-3.6\n",
      "creating build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/mask.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/coco.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/cocoeval.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "copying pycocotools/__init__.py -> build/lib.linux-x86_64-3.6/pycocotools\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "building 'pycocotools._mask' extension\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-3.6\n",
      "creating build/temp.linux-x86_64-3.6/pycocotools\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/numpy/core/include -I../common -I/opt/conda/include/python3.6m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.6/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
      "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
      "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
      "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
      "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
      "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
      "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
      "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
      "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
      "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
      "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
      "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
      "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
      "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
      "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
      "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
      "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
      "gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/opt/conda/lib/python3.6/site-packages/numpy/core/include -I../common -I/opt/conda/include/python3.6m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "gcc -pthread -shared -B /opt/conda/compiler_compat -L/opt/conda/lib -Wl,-rpath=/opt/conda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/../common/maskApi.o build/temp.linux-x86_64-3.6/pycocotools/_mask.o -o build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing pycocotools.egg-info/PKG-INFO\n",
      "writing dependency_links to pycocotools.egg-info/dependency_links.txt\n",
      "writing requirements to pycocotools.egg-info/requires.txt\n",
      "writing top-level names to pycocotools.egg-info/top_level.txt\n",
      "reading manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/mask.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/coco.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/cocoeval.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "copying build/lib.linux-x86_64-3.6/pycocotools/__init__.py -> build/bdist.linux-x86_64/egg/pycocotools\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/mask.py to mask.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/coco.py to coco.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/__init__.py to __init__.cpython-36.pyc\n",
      "creating stub loader for pycocotools/_mask.cpython-36m-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/pycocotools/_mask.py to _mask.cpython-36.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "pycocotools.__pycache__._mask.cpython-36: module references __file__\n",
      "creating 'dist/pycocotools-2.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing pycocotools-2.0-py3.6-linux-x86_64.egg\n",
      "creating /opt/conda/lib/python3.6/site-packages/pycocotools-2.0-py3.6-linux-x86_64.egg\n",
      "Extracting pycocotools-2.0-py3.6-linux-x86_64.egg to /opt/conda/lib/python3.6/site-packages\n",
      "Adding pycocotools 2.0 to easy-install.pth file\n",
      "\n",
      "Installed /opt/conda/lib/python3.6/site-packages/pycocotools-2.0-py3.6-linux-x86_64.egg\n",
      "Processing dependencies for pycocotools==2.0\n",
      "Searching for matplotlib==3.2.1\n",
      "Best match: matplotlib 3.2.1\n",
      "Adding matplotlib 3.2.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for Cython==0.29.16\n",
      "Best match: Cython 0.29.16\n",
      "Adding Cython 0.29.16 to easy-install.pth file\n",
      "Installing cygdb script to /opt/conda/bin\n",
      "Installing cython script to /opt/conda/bin\n",
      "Installing cythonize script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for setuptools==46.1.3.post20200325\n",
      "Best match: setuptools 46.1.3.post20200325\n",
      "Adding setuptools 46.1.3.post20200325 to easy-install.pth file\n",
      "Installing easy_install script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for cycler==0.10.0\n",
      "Best match: cycler 0.10.0\n",
      "Adding cycler 0.10.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for kiwisolver==1.2.0\n",
      "Best match: kiwisolver 1.2.0\n",
      "Adding kiwisolver 1.2.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for pyparsing==2.4.7\n",
      "Best match: pyparsing 2.4.7\n",
      "Adding pyparsing 2.4.7 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for python-dateutil==2.8.1\n",
      "Best match: python-dateutil 2.8.1\n",
      "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for numpy==1.18.2\n",
      "Best match: numpy 1.18.2\n",
      "Adding numpy 1.18.2 to easy-install.pth file\n",
      "Installing f2py script to /opt/conda/bin\n",
      "Installing f2py3 script to /opt/conda/bin\n",
      "Installing f2py3.6 script to /opt/conda/bin\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Searching for six==1.14.0\n",
      "Best match: six 1.14.0\n",
      "Adding six 1.14.0 to easy-install.pth file\n",
      "\n",
      "Using /opt/conda/lib/python3.6/site-packages\n",
      "Finished processing dependencies for pycocotools==2.0\n",
      "data/resized2014 exists\n",
      "data/vocab.pkl exists\n",
      "Processing /home/jovyan/.cache/pip/wheels/1c/07/d1/ff77968842daae1dde944173a8e8a7be193646d37842f13b24/efficientnet_pytorch-0.6.3-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: torch in /opt/conda/lib/python3.6/site-packages (from efficientnet-pytorch) (1.4.0)\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.6.3\n",
      "Collecting python-telegram-bot\n",
      "  Using cached python_telegram_bot-12.6.1-py2.py3-none-any.whl (371 kB)\n",
      "Requirement already satisfied: future>=0.16.0 in /opt/conda/lib/python3.6/site-packages (from python-telegram-bot) (0.18.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.6/site-packages (from python-telegram-bot) (2020.4.5.1)\n",
      "Requirement already satisfied: decorator>=4.4.0 in /opt/conda/lib/python3.6/site-packages (from python-telegram-bot) (4.4.2)\n",
      "Requirement already satisfied: tornado>=5.1 in /opt/conda/lib/python3.6/site-packages (from python-telegram-bot) (5.1.1)\n",
      "Requirement already satisfied: cryptography in /opt/conda/lib/python3.6/site-packages (from python-telegram-bot) (2.8)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.6/site-packages (from cryptography->python-telegram-bot) (1.14.0)\n",
      "Requirement already satisfied: six>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from cryptography->python-telegram-bot) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography->python-telegram-bot) (2.20)\n",
      "Installing collected packages: python-telegram-bot\n",
      "Successfully installed python-telegram-bot-12.6.1\n"
     ]
    }
   ],
   "source": [
    "! bash setup.sh\n",
    "! python3 -m pip install --upgrade efficientnet-pytorch\n",
    "! python3 -m pip install python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (4.45.0)\r\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from build_vocab import Vocabulary\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import matplotlib.pyplot as plt\n",
    "from models import EfficientNetBackbone, Encoder, Decoder\n",
    "import argparse\n",
    "\n",
    "from telegram import Bot\n",
    "\n",
    "bot = Bot('739462815:AAELJ6hhjzj3zv-3c_dkGkCyUS9vB3G5t90')\n",
    "    \n",
    "# class Attention\n",
    "    \n",
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        img_id = coco.anns[ann_id]['image_id']\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "#         caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "\n",
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers, collate_fn):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       json=json,\n",
    "                       vocab=vocab,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader, coco.coco\n",
    "\n",
    "def visualise_one(coco, encoder, decoder):\n",
    "    print('visualise one')\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    rand_idx = random.randint(0,len(coco.imgs)-1)\n",
    "    image_id = list(coco.imgs.keys())[rand_idx]\n",
    "    string_id = '0'*(6-len(str(image_id)))+str(image_id)\n",
    "    image = Image.open('../datasets/coco2014/train2014/COCO_train2014_000000{}.jpg'.format(string_id)).convert('RGB')\n",
    "    \n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    feature = encoder(image_tensor)\n",
    "    cell_state = feature.unsqueeze(0)\n",
    "    hidden_state = torch.zeros(cell_state.shape).to(device)\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "    \n",
    "    # Convert word_ids to words\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption[:-1])\n",
    "    plt.imshow(image)\n",
    "    plt.xlabel(sentence)\n",
    "    plt.savefig('sample.png')\n",
    "    plt.clf()\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "# hyperparams\n",
    "model_name = 'efficientnetb0-glove.200d-hidden200-connected_cell-input0'\n",
    "crop_size=224\n",
    "vocab_path = 'data/vocab.pkl'\n",
    "batch_size = 128\n",
    "embed_size = 200\n",
    "hidden_size = 512\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "num_workers = 2\n",
    "num_layers = 1\n",
    "log_step = 500\n",
    "save_step = 3000\n",
    "visualise_step = 200\n",
    "# save_step = 200\n",
    "image_dir = 'resized2014'\n",
    "model_path = './models'\n",
    "train_caption_path = '../datasets/coco2014/trainval_coco2014_captions/captions_train2014.json'\n",
    "test_caption_path = '../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json'\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "model_name = 'efficientnetb0-glove.200d-hidden200-connected_cell-input0'\n",
    "crop_size=224\n",
    "vocab_path = 'vocab.pkl'\n",
    "batch_size = 128\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "num_workers = 2\n",
    "num_layers = 1\n",
    "log_step = 500\n",
    "save_step = 3000\n",
    "visualise_step = 200\n",
    "# save_step = 200\n",
    "image_dir = 'resized2014'\n",
    "model_path = './models'\n",
    "train_caption_path = '../datasets/coco2014/trainval_coco2014_captions/captions_train2014.json'\n",
    "test_caption_path = '../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json'\n",
    "\n",
    "bot.sendMessage(-428968689,'Training {}'.format(model_name))\n",
    "\n",
    "efficientnetBackbone = EfficientNetBackbone.from_pretrained('efficientnet-b0')\n",
    "\n",
    "encoder = Encoder(efficientnetBackbone,efficientnetBackbone.output_size,hidden_size).to(device)\n",
    "decoder = Decoder(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "decoder.embed.load_state_dict(torch.load('pretrained/coco2014.glove.6B.200d.pth'))\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Create model directory\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Image preprocessing, normalization for the pretrained resnet\n",
    "transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(crop_size),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader\n",
    "train_data_loader, train_coco = get_loader(image_dir, train_caption_path, vocab, \n",
    "                         transform, batch_size,\n",
    "                         shuffle=True, num_workers=num_workers,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Train the models\n",
    "total_step = len(train_data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % log_step == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "            bot.sendMessage(-428968689,'Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "\n",
    "        if (i+1) % visualise_step == 0:\n",
    "            visualise_one(train_coco, encoder, decoder)\n",
    "            img = open('sample.png','rb')\n",
    "            bot.sendPhoto(-428968689,img)\n",
    "        \n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % save_step == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(\n",
    "                model_path, 'decoder.{}-{}-{}.pth'.format(model_name, epoch+1, i+1)))\n",
    "            torch.save(encoder.state_dict(), os.path.join(\n",
    "                model_path, 'encoder.{}-{}-{}.pth'.format(model_name, epoch+1, i+1)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def beam_search_decoding(feature,decoder):\n",
    "    \n",
    "    cell_state = feature.unsqueeze(0)\n",
    "    hidden_state = torch.zeros(cell_state.shape).to(device)\n",
    "    sampled_ids = decoder.sample(feature,(hidden_state,cell_state))\n",
    "    \n",
    "\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "# print(torch.load('models/encoder-5-3000.ckpt').keys())\n",
    "\n",
    "encoder.load_state_dict(torch.load('models/encoder.efficientnetb0-glove.200d-hidden200-connected_cell-5-3000.pth'))\n",
    "decoder.load_state_dict(torch.load('models/decoder.efficientnetb0-glove.200d-hidden200-connected_cell-5-3000.pth'))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "images = os.listdir('../datasets/coco2014/train2014/')\n",
    "\n",
    "for i in range(20):\n",
    "    image = Image.open('../datasets/coco2014/train2014/'+images[i]).convert('RGB')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    image_tensor = image.to(device).half()\n",
    "    feature = encoder(image_tensor)\n",
    "    \n",
    "    cell_state = feature.unsqueeze(0)\n",
    "    hidden_state = torch.zeros(cell_state.shape).to(device).half()\n",
    "    \n",
    "    sampled_ids = decoder.sample(feature,(hidden_state,cell_state))\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "    \n",
    "    # Convert word_ids to words\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption)\n",
    "    \n",
    "    # Print out the image and the generated caption\n",
    "    print (sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('ImageCaption@James.ipynb','{}.ipynb'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0053,  0.0126, -0.0050, -0.0141, -0.0043, -0.0035, -0.0143,  0.0058,\n",
      "         0.0106, -0.0111, -0.0116, -0.0167, -0.0055, -0.0285,  0.0296,  0.0062,\n",
      "         0.0131,  0.0043,  0.0024, -0.0010, -0.0060, -0.0090,  0.0013,  0.0123,\n",
      "        -0.0139, -0.0113, -0.0167,  0.0063, -0.0037,  0.0010, -0.0066, -0.0026,\n",
      "         0.0029,  0.0015,  0.0002,  0.0121, -0.0068,  0.0049,  0.0035,  0.0061,\n",
      "         0.0057, -0.0113,  0.0227,  0.0127,  0.0126,  0.0126, -0.0172, -0.0016,\n",
      "         0.0033,  0.0013, -0.0108,  0.0084,  0.0207, -0.0094,  0.0190,  0.0037,\n",
      "         0.0124,  0.0071, -0.0025, -0.0093,  0.0011,  0.0065,  0.0124, -0.0075,\n",
      "         0.0015,  0.0078, -0.0023, -0.0199, -0.0058,  0.0072, -0.0073, -0.0066,\n",
      "        -0.0173, -0.0125, -0.0120,  0.0043, -0.0076, -0.0091, -0.0053,  0.0010,\n",
      "        -0.0128, -0.0038,  0.0015, -0.0038, -0.0103, -0.0062, -0.0056,  0.0094,\n",
      "        -0.0045,  0.0069,  0.0238, -0.0024,  0.0016,  0.0140, -0.0028, -0.0035,\n",
      "        -0.0072, -0.0042, -0.0019, -0.0108, -0.0045, -0.0178, -0.0005,  0.0041,\n",
      "        -0.0082, -0.0052, -0.0042, -0.0142, -0.0006,  0.0111,  0.0075,  0.0093,\n",
      "        -0.0027,  0.0004,  0.0028,  0.0008,  0.0058,  0.0091,  0.0006, -0.0012,\n",
      "         0.0191,  0.0076,  0.0013,  0.0078,  0.0005, -0.0142, -0.0038, -0.0077,\n",
      "        -0.0102, -0.0127,  0.0158, -0.0108, -0.0019, -0.0063, -0.0128, -0.0121,\n",
      "        -0.0061, -0.0053, -0.0139, -0.0007,  0.0047,  0.0216, -0.0080,  0.0066,\n",
      "        -0.0095, -0.0045, -0.0132, -0.0022,  0.0134, -0.0201, -0.0135, -0.0137,\n",
      "         0.0051,  0.0138,  0.0033,  0.0080,  0.0143, -0.0046, -0.0143, -0.0009,\n",
      "        -0.0084, -0.0150, -0.0002,  0.0106,  0.0040,  0.0060,  0.0049,  0.0030,\n",
      "        -0.0186,  0.0020, -0.0032,  0.0193, -0.0092, -0.0003, -0.0075,  0.0137,\n",
      "        -0.0081, -0.0203, -0.0036,  0.0019, -0.0052, -0.0056,  0.0013,  0.0022,\n",
      "         0.0256, -0.0093, -0.0063,  0.0016, -0.0178, -0.0061,  0.0018,  0.0049,\n",
      "        -0.0145, -0.0156,  0.0036, -0.0003, -0.0173, -0.0026,  0.0080,  0.0080],\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "a = nn.Embedding(9956,200)\n",
    "print(a.weight.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<build_vocab.Vocabulary at 0x7f2b8fbda048>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco = COCO('../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9956"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0012, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a.weight.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Embedding(9956,200)\n",
    "a.load_state_dict(torch.load('pretrained/coco2014.glove.6B.200d.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4491, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a.weight.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9999, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = nn.Embedding(9956,200)\n",
    "b.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(256, 512, len(vocab), 1)\n",
    "decoder.load_state_dict(torch.load('models/decoder-5-3000.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0018, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.embed.weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(200, 512, len(vocab), 1)\n",
    "decoder.load_state_dict(torch.load('models/decoder.glove.200d-5-3000.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-32de9b1c0d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9956\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained/coco2014.glove.6B.200d.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    593\u001b[0m                 raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n\u001b[1;32m    594\u001b[0m                                 \u001b[0;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                 .format(torch.typename(value), name))\n\u001b[0m\u001b[1;32m    596\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "a = nn.Embedding(9956,200)\n",
    "a.load_state_dict(torch.load('pretrained/coco2014.glove.6B.200d.pth'))\n",
    "a.weight = a.weight/a.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.state_dict()['weight']/a.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nn.Embedding(9956,200,_weight=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(b.state_dict(),'pretrained/coco2014.glove.6B.200d.std1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_glove_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<build_vocab.Vocabulary at 0x7f98873775c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_glove_embedding(glove,'glove10.pkl',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
